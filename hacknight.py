# -*- coding: utf-8 -*-
"""Hacknight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vl7S2mUzqBgvCk26LOhtqho9Mcekh_WE
"""

from flask import Flask, jsonify, request

from collections import defaultdict
import nltk
# from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.tag import pos_tag
from gensim.parsing.preprocessing import STOPWORDS
from gensim.utils import simple_preprocess
from gensim import corpora
from gensim.models.ldamodel import LdaModel
import re

# app
app = Flask(__name__)


def lemmatize_stemming(text):
    # return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
    return WordNetLemmatizer().lemmatize(text, pos='v')    

def tokenize(text):
  result=[]
  for token in simple_preprocess(text):
    if token not in STOPWORDS:
      word = lemmatize_stemming(token)
      if(len(word)>3):
        result.append(word)
  return [result]


# doc = [tokenize(text)]

def preprocess(text):
  
  text = text.lower()
  text_tokens = tokenize(text)

  frequency = defaultdict(int)
  for text in text_tokens:
      for token in text:
          frequency[token] += 1

  return [[token for token in text if frequency[token] > 10] for text in text_tokens]


def get_model(text_data):
  dictionary = corpora.Dictionary(doc)
  corpus = [dictionary.doc2bow(text) for text in text_data]

  # fit LDA model
  return LdaModel(corpus=corpus,
                             id2word=dictionary,
                             num_topics=8,
                             passes=10)


def extract_topics(model):
  topics_dict = defaultdict(int)
  for i, topic in enumerate(model.print_topics(10)):
      list1 = re.findall('[a-z]+',topic[1])
      list2 = re.findall('\d.\d+',topic[1])
      for i in range(0,len(list1)):
        topics_dict[list1[i]]+=float(list2[i])

  topics = pos_tag(topics_dict.keys())

  noun_topics = [(topic,topics_dict[topic[0]]) for topic in topics if topic[1].find('VB')==-1 and topic[1].find('JJ')==-1]

  noun_topics = sorted(noun_topics,key = lambda x:x[1], reverse =  True)
  return [t[0][0] for t in noun_topics]

# routes
@app.route('/', methods=['POST'])
def predict():
  text_data = request.get_json(force=True)["data"]
  text_data = request.args.get('data')
  text = preprocess(speech)
  topic_list = extract_topics(get_model(text))
  return jsonify(results = topic_list)

if __name__ == '__main__':  
  nltk.download('punkt')
  nltk.download('wordnet')
  nltk.download('averaged_perceptron_tagger')
  app.run(port = 5000, debug=True)